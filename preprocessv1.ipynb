{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49e20415",
   "metadata": {},
   "source": [
    "Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a79643ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, OrdinalEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74a63fa",
   "metadata": {},
   "source": [
    "Cargamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "72cee79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./salario.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf928c",
   "metadata": {},
   "source": [
    "Visualizamos el dataframe inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "78197e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edad</th>\n",
       "      <th>trabajo</th>\n",
       "      <th>estudios</th>\n",
       "      <th>estado-civil</th>\n",
       "      <th>trabajo.1</th>\n",
       "      <th>posicion-familiar</th>\n",
       "      <th>etnia</th>\n",
       "      <th>sexo</th>\n",
       "      <th>ganancias_inversiones</th>\n",
       "      <th>perdidas_inversiones</th>\n",
       "      <th>horas-trabajo_semana</th>\n",
       "      <th>pais-origen</th>\n",
       "      <th>salario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>No-en-familia</td>\n",
       "      <td>Blanco</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Marido</td>\n",
       "      <td>Blanco</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>No-en-familia</td>\n",
       "      <td>Blanco</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Marido</td>\n",
       "      <td>Negro</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Negro</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27993</th>\n",
       "      <td>59</td>\n",
       "      <td>?</td>\n",
       "      <td>10th</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Soltero-a</td>\n",
       "      <td>Blanco</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27994</th>\n",
       "      <td>42</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Masters</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Marido</td>\n",
       "      <td>Blanco</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>62</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Sales</td>\n",
       "      <td>No-en-familia</td>\n",
       "      <td>Blanco</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>54</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Sales</td>\n",
       "      <td>No-en-familia</td>\n",
       "      <td>Blanco</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>35</td>\n",
       "      <td>Private</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Hijo-unico</td>\n",
       "      <td>Blanco</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27998 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       edad            trabajo    estudios         estado-civil  \\\n",
       "0        39          State-gov   Bachelors        Never-married   \n",
       "1        50   Self-emp-not-inc   Bachelors   Married-civ-spouse   \n",
       "2        38            Private     HS-grad             Divorced   \n",
       "3        53            Private        11th   Married-civ-spouse   \n",
       "4        28            Private   Bachelors   Married-civ-spouse   \n",
       "...     ...                ...         ...                  ...   \n",
       "27993    59                  ?        10th              Widowed   \n",
       "27994    42   Self-emp-not-inc     Masters   Married-civ-spouse   \n",
       "27995    62            Private     HS-grad              Widowed   \n",
       "27996    54            Private     HS-grad        Never-married   \n",
       "27997    35            Private     7th-8th        Never-married   \n",
       "\n",
       "                trabajo.1 posicion-familiar    etnia     sexo  \\\n",
       "0            Adm-clerical     No-en-familia   Blanco   Hombre   \n",
       "1         Exec-managerial            Marido   Blanco   Hombre   \n",
       "2       Handlers-cleaners     No-en-familia   Blanco   Hombre   \n",
       "3       Handlers-cleaners            Marido    Negro   Hombre   \n",
       "4          Prof-specialty             Mujer    Negro    Mujer   \n",
       "...                   ...               ...      ...      ...   \n",
       "27993                   ?         Soltero-a   Blanco    Mujer   \n",
       "27994     Exec-managerial            Marido   Blanco   Hombre   \n",
       "27995               Sales     No-en-familia   Blanco    Mujer   \n",
       "27996               Sales     No-en-familia   Blanco   Hombre   \n",
       "27997   Machine-op-inspct        Hijo-unico   Blanco    Mujer   \n",
       "\n",
       "       ganancias_inversiones  perdidas_inversiones  horas-trabajo_semana  \\\n",
       "0                       2174                     0                    40   \n",
       "1                          0                     0                    13   \n",
       "2                          0                     0                    40   \n",
       "3                          0                     0                    40   \n",
       "4                          0                     0                    40   \n",
       "...                      ...                   ...                   ...   \n",
       "27993                      0                     0                    40   \n",
       "27994                      0                     0                    50   \n",
       "27995                      0                     0                    43   \n",
       "27996                      0                     0                    40   \n",
       "27997                      0                     0                    30   \n",
       "\n",
       "          pais-origen salario  \n",
       "0       United-States   <=50K  \n",
       "1       United-States   <=50K  \n",
       "2       United-States   <=50K  \n",
       "3       United-States   <=50K  \n",
       "4                Cuba   <=50K  \n",
       "...               ...     ...  \n",
       "27993   United-States   <=50K  \n",
       "27994   United-States    >50K  \n",
       "27995   United-States   <=50K  \n",
       "27996   United-States   <=50K  \n",
       "27997   United-States   <=50K  \n",
       "\n",
       "[27998 rows x 13 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8f7bc5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27998 entries, 0 to 27997\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   edad                   27998 non-null  int64 \n",
      " 1   trabajo                27998 non-null  object\n",
      " 2   estudios               27998 non-null  object\n",
      " 3   estado-civil           27998 non-null  object\n",
      " 4   trabajo.1              27998 non-null  object\n",
      " 5   posicion-familiar      27998 non-null  object\n",
      " 6   etnia                  27998 non-null  object\n",
      " 7   sexo                   27998 non-null  object\n",
      " 8   ganancias_inversiones  27998 non-null  int64 \n",
      " 9   perdidas_inversiones   27998 non-null  int64 \n",
      " 10  horas-trabajo_semana   27998 non-null  int64 \n",
      " 11  pais-origen            27998 non-null  object\n",
      " 12  salario                27998 non-null  object\n",
      "dtypes: int64(4), object(9)\n",
      "memory usage: 2.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf061201",
   "metadata": {},
   "source": [
    "Vamos a ver posibles valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a2b7ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edad\n",
      "31    764\n",
      "23    763\n",
      "36    762\n",
      "35    756\n",
      "34    754\n",
      "     ... \n",
      "84     10\n",
      "83      6\n",
      "88      3\n",
      "85      2\n",
      "86      1\n",
      "Name: count, Length: 72, dtype: int64\n",
      "trabajo\n",
      "Private             19483\n",
      "Self-emp-not-inc     2211\n",
      "Local-gov            1810\n",
      "?                    1568\n",
      "State-gov            1119\n",
      "Self-emp-inc          961\n",
      "Federal-gov           830\n",
      "Without-pay            11\n",
      "Never-worked            5\n",
      "Name: count, dtype: int64\n",
      "estudios\n",
      "HS-grad         9033\n",
      "Some-college    6250\n",
      "Bachelors       4642\n",
      "Masters         1477\n",
      "Assoc-voc       1184\n",
      "11th            1023\n",
      "Assoc-acdm       905\n",
      "10th             811\n",
      "7th-8th          563\n",
      "Prof-school      488\n",
      "9th              438\n",
      "12th             369\n",
      "Doctorate        357\n",
      "5th-6th          282\n",
      "1st-4th          131\n",
      "Preschool         45\n",
      "Name: count, dtype: int64\n",
      "estado-civil\n",
      "Married-civ-spouse       12834\n",
      "Never-married             9172\n",
      "Divorced                  3837\n",
      "Separated                  899\n",
      "Widowed                    869\n",
      "Married-spouse-absent      366\n",
      "Married-AF-spouse           21\n",
      "Name: count, dtype: int64\n",
      "trabajo.1\n",
      "Prof-specialty       3566\n",
      "Craft-repair         3512\n",
      "Exec-managerial      3464\n",
      "Adm-clerical         3305\n",
      "Sales                3148\n",
      "Other-service        2857\n",
      "Machine-op-inspct    1712\n",
      "?                    1573\n",
      "Transport-moving     1372\n",
      "Handlers-cleaners    1146\n",
      "Farming-fishing       859\n",
      "Tech-support          791\n",
      "Protective-serv       552\n",
      "Priv-house-serv       133\n",
      "Armed-Forces            8\n",
      "Name: count, dtype: int64\n",
      "posicion-familiar\n",
      "Marido           11300\n",
      "No-en-familia     7186\n",
      "Hijo-unico        4375\n",
      "Soltero-a         2953\n",
      "Mujer             1353\n",
      "Otro-familiar      831\n",
      "Name: count, dtype: int64\n",
      "etnia\n",
      "Blanco                      23920\n",
      "Negro                        2685\n",
      "Asiatico-Pacifico-Isleno      887\n",
      "Amerindio-esquimal            270\n",
      "Otro                          236\n",
      "Name: count, dtype: int64\n",
      "sexo\n",
      "Hombre    18738\n",
      "Mujer      9260\n",
      "Name: count, dtype: int64\n",
      "ganancias_inversiones\n",
      "0        25663\n",
      "15024      297\n",
      "7688       236\n",
      "7298       208\n",
      "99999      142\n",
      "         ...  \n",
      "1173         1\n",
      "6097         1\n",
      "6723         1\n",
      "1639         1\n",
      "7978         1\n",
      "Name: count, Length: 117, dtype: int64\n",
      "perdidas_inversiones\n",
      "0       26698\n",
      "1902      172\n",
      "1977      148\n",
      "1887      124\n",
      "1485       45\n",
      "        ...  \n",
      "2201        1\n",
      "1944        1\n",
      "2467        1\n",
      "2163        1\n",
      "2754        1\n",
      "Name: count, Length: 90, dtype: int64\n",
      "horas-trabajo_semana\n",
      "40    13154\n",
      "50     2403\n",
      "45     1569\n",
      "60     1264\n",
      "35     1115\n",
      "      ...  \n",
      "94        1\n",
      "82        1\n",
      "92        1\n",
      "74        1\n",
      "95        1\n",
      "Name: count, Length: 94, dtype: int64\n",
      "pais-origen\n",
      "United-States                 25092\n",
      "Mexico                          554\n",
      "?                               495\n",
      "Philippines                     165\n",
      "Germany                         116\n",
      "Canada                          107\n",
      "Puerto-Rico                     105\n",
      "El-Salvador                      85\n",
      "England                          85\n",
      "Cuba                             82\n",
      "India                            82\n",
      "South                            74\n",
      "China                            67\n",
      "Jamaica                          66\n",
      "Vietnam                          58\n",
      "Italy                            58\n",
      "Dominican-Republic               57\n",
      "Guatemala                        56\n",
      "Japan                            55\n",
      "Poland                           54\n",
      "Columbia                         50\n",
      "Taiwan                           47\n",
      "Iran                             40\n",
      "Haiti                            38\n",
      "Nicaragua                        30\n",
      "Portugal                         30\n",
      "Greece                           26\n",
      "Peru                             25\n",
      "France                           25\n",
      "Ireland                          22\n",
      "Ecuador                          22\n",
      "Thailand                         18\n",
      "Cambodia                         17\n",
      "Laos                             13\n",
      "Hong                             13\n",
      "Trinadad&Tobago                  13\n",
      "Honduras                         12\n",
      "Yugoslavia                       12\n",
      "Hungary                          12\n",
      "Scotland                         10\n",
      "Outlying-US(Guam-USVI-etc)        9\n",
      "Holand-Netherlands                1\n",
      "Name: count, dtype: int64\n",
      "salario\n",
      "<=50K    21290\n",
      ">50K      6708\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(df[i].value_counts().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4758377e",
   "metadata": {},
   "source": [
    "Vemos que hay 3 columnas (trabajo, trabajo.1 y pais-origen) que tienen '?' como uno de los valores más repetidos. Vamos a cambiar este valor a valores nulos para poder tratarlos.  \n",
    "Para ello, volvemos a cargar el dataset de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6d394c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27998 entries, 0 to 27997\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   edad                   27998 non-null  int64 \n",
      " 1   trabajo                26430 non-null  object\n",
      " 2   estudios               27998 non-null  object\n",
      " 3   estado-civil           27998 non-null  object\n",
      " 4   trabajo.1              26425 non-null  object\n",
      " 5   posicion-familiar      27998 non-null  object\n",
      " 6   etnia                  27998 non-null  object\n",
      " 7   sexo                   27998 non-null  object\n",
      " 8   ganancias_inversiones  27998 non-null  int64 \n",
      " 9   perdidas_inversiones   27998 non-null  int64 \n",
      " 10  horas-trabajo_semana   27998 non-null  int64 \n",
      " 11  pais-origen            27503 non-null  object\n",
      " 12  salario                27998 non-null  object\n",
      "dtypes: int64(4), object(9)\n",
      "memory usage: 2.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./salario.csv', na_values=['?'], skipinitialspace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c846780",
   "metadata": {},
   "source": [
    "Para evitar data leakage durante el preprocesado, haremos el train-validation split ahora.  \n",
    "También, mapeamos el target para trabajar con 0 y 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "203f9269",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapa_salario = {\n",
    "    '<=50K': 0,\n",
    "    '>50K': 1\n",
    "}\n",
    "\n",
    "df['salario'] = df['salario'].map(mapa_salario)\n",
    "\n",
    "\n",
    "target_col = 'salario'\n",
    "y = df[target_col]\n",
    "X = df.drop(target_col, axis=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a244a3f",
   "metadata": {},
   "source": [
    "Comprobamos el split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f84f865e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de X_train: (22398, 12)\n",
      "Tamaño de X_val: (5600, 12)\n",
      "\n",
      "Proporción en y_train:\n",
      "salario\n",
      "0    0.760425\n",
      "1    0.239575\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Proporción en y_val:\n",
      "salario\n",
      "0    0.760357\n",
      "1    0.239643\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tamaño de X_train: {X_train.shape}\")\n",
    "print(f\"Tamaño de X_val: {X_val.shape}\")\n",
    "print(\"\\nProporción en y_train:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nProporción en y_val:\")\n",
    "print(y_val.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db504a32",
   "metadata": {},
   "source": [
    "Para los nulos, decidimos imputar valores ya que hay una cantidad de nulos considerables y si eliminaramos tanto filas como columnas estaríamos perdiendo información valiosa.  \n",
    "Para ello, veamos que columnas tienen valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c2ac1346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edad                        0\n",
      "trabajo                  1249\n",
      "estudios                    0\n",
      "estado-civil                0\n",
      "trabajo.1                1253\n",
      "posicion-familiar           0\n",
      "etnia                       0\n",
      "sexo                        0\n",
      "ganancias_inversiones       0\n",
      "perdidas_inversiones        0\n",
      "horas-trabajo_semana        0\n",
      "pais-origen               396\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeafbe55",
   "metadata": {},
   "source": [
    "Imputamos con la moda al ser columnas de tipo 'object'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e28e504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_con_nulos = ['trabajo', 'trabajo.1', 'pais-origen']\n",
    "\n",
    "for col in columnas_con_nulos:\n",
    "    moda = X_train[col].mode()[0]\n",
    "    X_train[col] = X_train[col].fillna(moda)\n",
    "    X_val[col] = X_val[col].fillna(moda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22bddea",
   "metadata": {},
   "source": [
    "Comprobamos que se han imputado correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "69687940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulos restantes en X_train:\n",
      "edad                     0\n",
      "trabajo                  0\n",
      "estudios                 0\n",
      "estado-civil             0\n",
      "trabajo.1                0\n",
      "posicion-familiar        0\n",
      "etnia                    0\n",
      "sexo                     0\n",
      "ganancias_inversiones    0\n",
      "perdidas_inversiones     0\n",
      "horas-trabajo_semana     0\n",
      "pais-origen              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Nulos restantes en X_train:\")\n",
    "print(X_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca4c637",
   "metadata": {},
   "source": [
    "Ahora vamos a tratar las columnas para poder entrenar el modelo:\n",
    "\n",
    "Para las columnas categóricas, hemos identificado tres tipos distintos de variables categóricas en nuestro dataset y, para maximizar la precisión del modelo, aplicaremos una estrategia de codificación específica para cada una:\n",
    "\n",
    "- Variables Ordinales (con orden): La columna estudios tiene una jerarquía clara (ej. HS-grad < Bachelors < Doctorate). Para conservar esta valiosa información, usaremos un OrdinalEncoder con un mapa de orden definido, convirtiéndola en una única columna numérica (ej. 0 a 15).\n",
    "\n",
    "- Variables Binarias (dos valores): La columna sexo solo tiene dos valores. Usar OneHotEncoder sería ineficiente. Aplicaremos un OrdinalEncoder (o .map()) para convertirla en una sola columna (0 o 1).\n",
    "\n",
    "- Variables Nominales (sin orden): El resto de columnas como trabajo, estado-civil y pais-origen son etiquetas sin orden lógico. Para estas, usaremos OneHotEncoder, que es la única forma correcta de evitar que el modelo aprenda patrones falsos (ej. creer que \"Mexico\" > \"Cuba\").\n",
    "\n",
    "En cuanto a las columnas numericas:\n",
    "\n",
    "- Hay columnas como ganancias_inversiones, con muchísimos ceros y unos pocos valores altísimos (como 99999).\n",
    "    No sabemos si ese 99999 es un error o si es información valiosa (ej. un código para \"ganancias máximas\").\n",
    "\n",
    "    - Opción 1: Borrar outliers y usar StandardScaler\n",
    "        Encontrar el 99999, borrarlo o cambiarlo por un valor \"normal\", y luego usar StandardScaler (que usa la media).\n",
    "        El Riesgo: Si 99999 era un dato valioso, acabamos de destruir la información más importante de esa columna. La media ahora será incorrecta.\n",
    "\n",
    "    - Opción 2: Usar RobustScaler (La que elegimos)\n",
    "        Qué es: RobustScaler usa la mediana para escalar, no la media.\n",
    "        La Ventaja: La mediana de nuestra columna es 0. Al RobustScaler no le importa el valor 99999 para calcular la escala, así que no se \"contamina\".\n",
    "\n",
    "    Si 99999 era un error: RobustScaler lo ignoró.  \n",
    "    Si 99999 era valioso: RobustScaler lo mantiene como un valor muy alto y único. El modelo aún puede verlo y aprender de él. ¡Perfecto!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b6e7b87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones (originales): (22398, 12)\n",
      "Nuevas dimensiones (preprocesadas): (22398, 87)\n"
     ]
    }
   ],
   "source": [
    "numeric_features = ['edad', 'ganancias_inversiones', 'perdidas_inversiones', 'horas-trabajo_semana']\n",
    "ordinal_features = ['estudios']\n",
    "binary_features = ['sexo']\n",
    "nominal_features = ['trabajo', 'estado-civil', 'trabajo.1', 'posicion-familiar', 'etnia', 'pais-origen']\n",
    "education_order = [\n",
    "    'Preschool', '1st-4th', '5th-6th', '7th-8th', '9th', \n",
    "    '10th', '11th', '12th', 'HS-grad', 'Some-college', \n",
    "    'Assoc-voc', 'Assoc-acdm', 'Bachelors', 'Masters', \n",
    "    'Prof-school', 'Doctorate'\n",
    "]\n",
    "\n",
    "numeric_transformer = RobustScaler()\n",
    "ordinal_transformer = OrdinalEncoder(\n",
    "    categories=[education_order],\n",
    "    handle_unknown='use_encoded_value',\n",
    "    unknown_value=-1\n",
    ")\n",
    "binary_transformer = OrdinalEncoder()\n",
    "nominal_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('ord', ordinal_transformer, ordinal_features),\n",
    "        ('bin', binary_transformer, binary_features),\n",
    "        ('nom', nominal_transformer, nominal_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = preprocessor.transform(X_val)\n",
    "\n",
    "print(f\"Dimensiones (originales): {X_train.shape}\")\n",
    "print(f\"Nuevas dimensiones (preprocesadas): {X_train_preprocessed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07848ed3",
   "metadata": {},
   "source": [
    "Como tenemos datos desbalanceados, elegimos SMOTE porque es la mejor manera de equilibrar los datos sin perjudicar al modelo.  \n",
    "Las otras opciones eran peores: \n",
    "- Undersampling (borrar datos) significaba perder información valiosa.  \n",
    "- Random Oversampling (copiar datos) provoca que el modelo memorice y se sobreajuste (overfitting).\n",
    "- SMOTE es la mejor solución porque, en lugar de copiar, crea nuevos datos sintéticos que son parecidos a los originales. Esto nos permite balancear el dataset para que el modelo aprenda mejor, pero sin perder información ni caer en el sobreajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ad12edc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de X_train (antes de SMOTE): (22398, 87)\n",
      "Dimensiones de X_train (después de SMOTE): (34064, 87)\n",
      "\n",
      "Balance de clases (antes de SMOTE):\n",
      "salario\n",
      "0    17032\n",
      "1     5366\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Balance de clases (después de SMOTE):\n",
      "salario\n",
      "0    17032\n",
      "1    17032\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(\n",
    "    X_train_preprocessed, \n",
    "    y_train\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Dimensiones de X_train (antes de SMOTE): {X_train_preprocessed.shape}\")\n",
    "print(f\"Dimensiones de X_train (después de SMOTE): {X_train_balanced.shape}\")\n",
    "\n",
    "print(\"\\nBalance de clases (antes de SMOTE):\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"\\nBalance de clases (después de SMOTE):\")\n",
    "print(y_train_balanced.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad911749",
   "metadata": {},
   "source": [
    "Pasemos con la selección de características:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27e4fe9",
   "metadata": {},
   "source": [
    "Métodos de filtro:\n",
    "- Hemos elegido f_classif (f-score) como nuestro método de filtro porque se alinea perfectamente con las recomendaciones de la asignatura para nuestro problema.  \n",
    "- En los apuntes se indica que el f-score es ideal para problemas de clase binaria, que es exactamente nuestro caso (salario >50K o <=50K).  \n",
    "- Dado que nuestro preprocesado ha convertido las 103 características en datos numéricos, f_classif es la herramienta estadística idónea para puntuar cada característica midiendo la diferencia de sus medias entre las dos clases de salario.  \n",
    "- Esto lo hace más apropiado que el Chi-cuadrado, que es para datos categóricos , o la correlación de Pearson, que solo mide relaciones lineales. \n",
    "\n",
    "- La selección del número de características, $k$, es en sí misma un hiperparámetro. Para realizar la evaluación comparativa de los tres métodos , fijamos un valor heurístico de $k=30$.  \n",
    "- Este valor se eligió por dos motivos:  \n",
    "    - Representa una reducción significativa (de 103 a 30 características), lo cual pone a prueba la capacidad de los métodos para eliminar ruido.\n",
    "    - Establece una base de comparación equitativa para evaluar la calidad de las características seleccionadas por cada uno de los tres enfoques, más que la cantidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "beb4f9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índices de las características elegidas: (array([ 0,  1,  2,  3,  4,  5,  9, 10, 14, 16, 18, 19, 20, 21, 24, 25, 26,\n",
      "       27, 28, 30, 35, 36, 37, 38, 39, 40, 43, 44, 71, 84]),)\n"
     ]
    }
   ],
   "source": [
    "k_best_selector = SelectKBest(score_func=f_classif, k=30)\n",
    "k_best_selector.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "feature_scores_filter = k_best_selector.scores_\n",
    "features_elegidas_filter = k_best_selector.get_support()\n",
    "\n",
    "print(f\"Índices de las características elegidas: {np.where(features_elegidas_filter)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beb44aa",
   "metadata": {},
   "source": [
    "Métodos wrapper:\n",
    "- Elegimos RFE porque es un método Wrapper potente que sí analiza las interacciones, y usamos LogisticRegression dentro de él porque es un modelo muy rápido que nos permite ejecutar RFE en un tiempo razonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d4ec4441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índices de las características elegidas: (array([ 5, 13, 16, 24, 25, 26, 28, 29, 30, 33, 35, 37, 39, 45, 48, 49, 51,\n",
      "       52, 53, 58, 70, 71, 72, 73, 74, 78, 80, 83, 85, 86]),)\n"
     ]
    }
   ],
   "source": [
    "estimator = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
    "rfe_selector = RFE(estimator=estimator, n_features_to_select=30, step=1)\n",
    "\n",
    "rfe_selector.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "features_elegidas_wrapper = rfe_selector.support_\n",
    "\n",
    "print(f\"Índices de las características elegidas: {np.where(rfe_selector.ranking_ == 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2d35f1",
   "metadata": {},
   "source": [
    "Métodos embebidos:\n",
    "- Hemos seleccionado LASSO (Regularización L1) como nuestro método embebido porque, de las técnicas de regularización presentadas en la asignatura, es la única diseña específicamente para la selección de características. \n",
    "-  Los apuntes también mencionan Ridge (Regularización L2), pero este método solo reduce el peso de las características (usando el cuadrado de los pesos) sin forzarlas a ser exactamente cero.  \n",
    "- LASSO (L1), en cambio, sí \"elimina aquellas con menor valor\" al forzar sus coeficientes a cero, realizando así una verdadera selección de características en un único y eficiente paso de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "16eb07a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índices de las características elegidas: (array([ 0,  5,  7, 11, 12, 14, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28,\n",
      "       29, 30, 34, 35, 36, 37, 38, 39, 40, 41, 45, 71, 75]),)\n"
     ]
    }
   ],
   "source": [
    "lasso_model = LogisticRegression(\n",
    "    penalty='l1', \n",
    "    solver='liblinear', \n",
    "    C=0.1, \n",
    "    random_state=42,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "embedded_selector = SelectFromModel(\n",
    "    lasso_model, \n",
    "    max_features=30,\n",
    "    threshold=-np.inf\n",
    ") \n",
    "\n",
    "embedded_selector.fit(X_train_balanced, y_train_balanced)\n",
    "features_elegidas_embedded = embedded_selector.get_support()\n",
    "\n",
    "print(f\"Índices de las características elegidas: {np.where(features_elegidas_embedded)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f893f2cb",
   "metadata": {},
   "source": [
    "Usaremos LogisticRegresion para ver cuál es la mejor opción entrando y viendo el f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "15251736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (87 features):      0.6828\n",
      "Método 'Filtro' (30 features):   0.6698\n",
      "Método 'Wrapper' (30 features):  0.6265\n",
      "Método 'Embebido' (30 features): 0.6310\n",
      "Consenso 'Al menos 2' (23 features): 0.6312\n",
      "Consenso 'Todos 3' (10 features):   0.5731\n"
     ]
    }
   ],
   "source": [
    "model_juez = LogisticRegression(solver='liblinear', random_state=32)\n",
    "\n",
    "features_consenso_2 = (features_elegidas_filter & features_elegidas_wrapper) | \\\n",
    "                      (features_elegidas_filter & features_elegidas_embedded) | \\\n",
    "                      (features_elegidas_wrapper & features_elegidas_embedded) # Que estén en al menos 2 de los 3 tipos\n",
    "                      \n",
    "features_consenso_3 = (features_elegidas_filter & features_elegidas_wrapper & features_elegidas_embedded) # Que estén en los 3\n",
    "\n",
    "num_consenso_2 = np.sum(features_consenso_2)\n",
    "num_consenso_3 = np.sum(features_consenso_3)\n",
    "\n",
    "\n",
    "# Baseline (Todas las 87)\n",
    "model_juez.fit(X_train_balanced, y_train_balanced)\n",
    "preds_all = model_juez.predict(X_val_preprocessed)\n",
    "f1_all = f1_score(y_val, preds_all)\n",
    "print(f\"Baseline (87 features):      {f1_all:.4f}\")\n",
    "\n",
    "# Filtro\n",
    "X_train_fil = X_train_balanced[:, features_elegidas_filter]\n",
    "X_val_fil = X_val_preprocessed[:, features_elegidas_filter]\n",
    "model_juez.fit(X_train_fil, y_train_balanced)\n",
    "preds_fil = model_juez.predict(X_val_fil)\n",
    "f1_filtro = f1_score(y_val, preds_fil)\n",
    "print(f\"Método 'Filtro' (30 features):   {f1_filtro:.4f}\")\n",
    "\n",
    "# Wrapper\n",
    "X_train_wra = X_train_balanced[:, features_elegidas_wrapper]\n",
    "X_val_wra = X_val_preprocessed[:, features_elegidas_wrapper]\n",
    "model_juez.fit(X_train_wra, y_train_balanced)\n",
    "preds_wra = model_juez.predict(X_val_wra)\n",
    "f1_wrapper = f1_score(y_val, preds_wra)\n",
    "print(f\"Método 'Wrapper' (30 features):  {f1_wrapper:.4f}\")\n",
    "\n",
    "# Embebido\n",
    "X_train_emb = X_train_balanced[:, features_elegidas_embedded]\n",
    "X_val_emb = X_val_preprocessed[:, features_elegidas_embedded]\n",
    "model_juez.fit(X_train_emb, y_train_balanced)\n",
    "preds_emb = model_juez.predict(X_val_emb)\n",
    "f1_embebido = f1_score(y_val, preds_emb)\n",
    "print(f\"Método 'Embebido' (30 features): {f1_embebido:.4f}\")\n",
    "\n",
    "# En al menos 2\n",
    "X_train_c2 = X_train_balanced[:, features_consenso_2]\n",
    "X_val_c2 = X_val_preprocessed[:, features_consenso_2]\n",
    "model_juez.fit(X_train_c2, y_train_balanced)\n",
    "preds_c2 = model_juez.predict(X_val_c2)\n",
    "f1_c2 = f1_score(y_val, preds_c2)\n",
    "print(f\"Consenso 'Al menos 2' ({num_consenso_2} features): {f1_c2:.4f}\")\n",
    "\n",
    "# En los 3\n",
    "X_train_c3 = X_train_balanced[:, features_consenso_3]\n",
    "X_val_c3 = X_val_preprocessed[:, features_consenso_3]\n",
    "model_juez.fit(X_train_c3, y_train_balanced)\n",
    "preds_c3 = model_juez.predict(X_val_c3)\n",
    "f1_c3 = f1_score(y_val, preds_c3)\n",
    "print(f\"Consenso 'Todos 3' ({num_consenso_3} features):   {f1_c3:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ddf9f",
   "metadata": {},
   "source": [
    "Aunque el método de Filtro (30) ofreció un F1-score ligeramente superior (0.6698), se ha optado por el conjunto de 23 características ('Consenso Al menos 2'). Se ha priorizado la robustez del modelo, ya que este conjunto representa las características en las que al menos dos de los tres métodos de selección coincidieron, eliminando así características menos fiables y creando un dataset más estable para la comparativa general de modelos.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b26620c",
   "metadata": {},
   "source": [
    "Exportamos en el formato pedido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "431a3754",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = X_train_balanced[:, features_consenso_2]\n",
    "X_val_final = X_val_preprocessed[:, features_consenso_2]\n",
    "\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "final_feature_names = feature_names[features_consenso_2]\n",
    "\n",
    "df_train = pd.DataFrame(X_train_final.toarray(), columns=final_feature_names)\n",
    "df_train['salario'] = y_train_balanced.values\n",
    "\n",
    "df_val = pd.DataFrame(X_val_final.toarray(), columns=final_feature_names)\n",
    "df_val['salario'] = y_val.values\n",
    "\n",
    "df_processed_final = pd.concat([df_train, df_val], axis=0)\n",
    "compression_opts = dict(method='zip', archive_name='processed_dataset.csv')\n",
    "df_processed_final.to_csv(\n",
    "    'processed_dataset.csv.zip', \n",
    "    index=False, \n",
    "    compression=compression_opts\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
